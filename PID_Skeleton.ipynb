{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNaNSwtOGAJH5vlyUEurHYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pratimamau/PID/blob/main/PID_Skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1tEks_Wog93A",
        "outputId": "fe553273-8767-4ec5-e371-50056fc39e0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PID'...\n",
            "remote: Enumerating objects: 258, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 258 (delta 25), reused 70 (delta 16), pack-reused 165\u001b[K\n",
            "Receiving objects: 100% (258/258), 113.32 MiB | 16.67 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Pratimamau/PID"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to install dependencies\n",
        "%cd PID/tf-pose-estimation\n",
        "! pip3 install -r requirements.txt\n",
        "%cd tf_pose/pafprocess\n",
        "! sudo apt install swig\n",
        "!swig -python -c++ pafprocess.i && python3 setup.py build_ext --inplace"
      ],
      "metadata": {
        "id": "endm7enChUVo",
        "outputId": "59335ace-bfa2-4c04-ab54-d82e8b0e1e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PID/tf-pose-estimation\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ppwwyyxx/tensorpack.git (from -r requirements.txt (line 13))\n",
            "  Cloning https://github.com/ppwwyyxx/tensorpack.git to /tmp/pip-req-build-5g5fnxje\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ppwwyyxx/tensorpack.git /tmp/pip-req-build-5g5fnxje\n",
            "  Resolved https://github.com/ppwwyyxx/tensorpack.git to commit a9a2660d9ebe8fe4f32693b59f1e003687716d81\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (0.56.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 7)) (2.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 8)) (2.27.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 9)) (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (1.10.1)\n",
            "Collecting slidingwindow\n",
            "  Downloading slidingwindow-0.0.14-py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 12)) (4.65.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->-r requirements.txt (line 3)) (2.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.39.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.22.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->-r requirements.txt (line 5)) (67.7.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->-r requirements.txt (line 5)) (0.39.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->-r requirements.txt (line 8)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->-r requirements.txt (line 8)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->-r requirements.txt (line 8)) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->-r requirements.txt (line 8)) (1.26.15)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.25.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirements.txt (line 9)) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.9/dist-packages (from tensorpack==0.11->-r requirements.txt (line 13)) (0.8.10)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from tensorpack==0.11->-r requirements.txt (line 13)) (1.0.5)\n",
            "Collecting msgpack-numpy>=0.4.4.2\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.9/dist-packages (from tensorpack==0.11->-r requirements.txt (line 13)) (23.2.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 4)) (3.15.0)\n",
            "Building wheels for collected packages: fire, tensorpack\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=9caace0e5ca3862f63ec51a5f765e2b1fa0f416ce1de22b253e9c0761ad97b3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "  Building wheel for tensorpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorpack: filename=tensorpack-0.11-py2.py3-none-any.whl size=296939 sha256=5de717102d46f9cbe05395028a6eb845ab3f809c6900bd874f89f321880fe5d0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u07pojrp/wheels/13/68/61/fbea2894d8559735e06c5b3a07bb662759506c4e4cd5261c8c\n",
            "Successfully built fire tensorpack\n",
            "Installing collected packages: argparse, slidingwindow, msgpack-numpy, fire, dill, tensorpack\n",
            "Successfully installed argparse-1.4.0 dill-0.3.6 fire-0.5.0 msgpack-numpy-0.4.8 slidingwindow-0.0.14 tensorpack-0.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PID/tf-pose-estimation/tf_pose/pafprocess\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 1,086 kB of archives.\n",
            "After this operation, 5,413 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1,081 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5,528 B]\n",
            "Fetched 1,086 kB in 2s (560 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.1-5build1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\n",
            "Unpacking swig (4.0.1-5build1) ...\n",
            "Setting up swig4.0 (4.0.1-5build1) ...\n",
            "Setting up swig (4.0.1-5build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "running build_ext\n",
            "building '_pafprocess' extension\n",
            "swigging pafprocess.i to pafprocess_wrap.cpp\n",
            "swig -python -c++ -o pafprocess_wrap.cpp pafprocess.i\n",
            "creating build/temp.linux-x86_64-3.9\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.9/dist-packages/numpy/core/include -I. -I/usr/include/python3.9 -c pafprocess.cpp -o build/temp.linux-x86_64-3.9/pafprocess.o\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.9/dist-packages/numpy/core/include -I. -I/usr/include/python3.9 -c pafprocess_wrap.cpp -o build/temp.linux-x86_64-3.9/pafprocess_wrap.o\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.9/pafprocess.o build/temp.linux-x86_64-3.9/pafprocess_wrap.o -o /content/PID/tf-pose-estimation/tf_pose/pafprocess/_pafprocess.cpython-39-x86_64-linux-gnu.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run this to access the weight files from the drive link shared \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-9XsxtTwhFF5",
        "outputId": "ec72d26c-5eb2-40c6-e094-4f679acc903f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU6qWoWr1QpP",
        "outputId": "45479719-56be-406e-ca32-2991dcbe5999",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run this\n",
        "%cd /content/PID\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import glob\n",
        " \n",
        "import sys #Run this\n",
        "from absl import app, logging, flags\n",
        "from absl.flags import FLAGS\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from yolov3_tf2.models import (\n",
        "    YoloV3, YoloV3Tiny\n",
        ")\n",
        "from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n",
        "from yolov3_tf2.utils import draw_outputs\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "flags.DEFINE_string('classes', 'data/coco.names', 'path to classes file')\n",
        "flags.DEFINE_string('weights', '/content/drive/My Drive/datax_volvo_additional_files/yolov3_train_5.tf','path to weights file')\n",
        "flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n",
        "flags.DEFINE_integer('size', 416, 'resize images to')\n",
        "flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n",
        "flags.DEFINE_integer('num_classes', 1, 'number of classes in the model')\n",
        "flags.DEFINE_string('video', 'data/video_0016.mp4','path to video file or number for webcam)')\n",
        "flags.DEFINE_string('output','Result_model_D.mp4', 'path to output video')\n",
        "flags.DEFINE_string('output_format', 'mp4v', 'codec used in VideoWriter when saving video to file')\n",
        "\n",
        "app._run_init(['yolov3'], app.parse_flags_with_usage)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PID\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yolov3']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HEsISlq9h9im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsuZt3UXTH_x",
        "outputId": "6e437422-4b52-495f-915b-8dda1e7fdbc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/PID/deep_sort\n",
        "from ds_tools.generate_detections import create_box_encoder\n",
        "from ds_application_util import preprocessing\n",
        "from ds_deep_sort import nn_matching\n",
        "from ds_deep_sort.detection import Detection\n",
        "from ds_deep_sort.tracker import Tracker\n",
        "\n",
        "%cd /content/PID/tf-pose-estimation\n",
        "from tf_pose.estimator import TfPoseEstimator\n",
        "from tf_pose.networks import get_graph_path, model_wh\n",
        "from tf_pose.estimator import Human\n",
        "model = TfPoseEstimator(get_graph_path('egen_jaad_1_5'), target_size=(100, 100))\n",
        "\n",
        "%cd /content/PID\n",
        "nms_max_overlap = 1.0\n",
        "max_cosine_distance = 0.2\n",
        "nn_budget = None\n",
        "encoder = create_box_encoder('mars-small128.pb', batch_size=32)\n",
        "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
        "tracker = Tracker(metric)\n",
        "\n",
        "with open('densenet_model.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "\n",
        "model_j = tf.keras.models.model_from_json(json_savedModel)\n",
        "model_j.load_weights('densenet_2.hdf5')\n",
        "\n",
        "def pred_func(X_test):\n",
        "  predictions = model_j.predict(X_test[0:1], verbose=0)\n",
        "  Y = np.argmax(predictions[0], axis=0)\n",
        "    \n",
        "  return Y\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0425 09:41:17.964741 140561405978432 deprecation.py:364] From /usr/local/lib/python3.9/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PID/deep_sort\n",
            "/content/PID/tf-pose-estimation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-04-25 09:41:19,063] [TfPoseEstimator] [INFO] loading graph from /content/drive/My Drive/datax_volvo_additional_files/graph_opt.pb(default size=100x100)\n",
            "I0425 09:41:19.063195 140561405978432 estimator.py:320] loading graph from /content/drive/My Drive/datax_volvo_additional_files/graph_opt.pb(default size=100x100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TfPoseEstimator/preprocess_divide/y\n",
            "TfPoseEstimator/preprocess_subtract/y\n",
            "TfPoseEstimator/concat_stage2/axis\n",
            "TfPoseEstimator/concat_stage3/axis\n",
            "TfPoseEstimator/concat_stage4/axis\n",
            "TfPoseEstimator/concat_stage5/axis\n",
            "TfPoseEstimator/concat_stage6/axis\n",
            "TfPoseEstimator/Openpose/concat_stage7/axis\n",
            "TfPoseEstimator/Mconv1_stage2_L1/biases/read/_0__cf__0\n",
            "TfPoseEstimator/Mconv1_stage2_L1/weights/read/_1__cf__1\n",
            "TfPoseEstimator/Mconv1_stage2_L2/biases/read/_2__cf__2\n",
            "TfPoseEstimator/Mconv1_stage2_L2/weights/read/_3__cf__3\n",
            "TfPoseEstimator/Mconv1_stage3_L1/biases/read/_4__cf__4\n",
            "TfPoseEstimator/Mconv1_stage3_L1/weights/read/_5__cf__5\n",
            "TfPoseEstimator/Mconv1_stage3_L2/biases/read/_6__cf__6\n",
            "TfPoseEstimator/Mconv1_stage3_L2/weights/read/_7__cf__7\n",
            "TfPoseEstimator/Mconv1_stage4_L1/biases/read/_8__cf__8\n",
            "TfPoseEstimator/Mconv1_stage4_L1/weights/read/_9__cf__9\n",
            "TfPoseEstimator/Mconv1_stage4_L2/biases/read/_10__cf__10\n",
            "TfPoseEstimator/Mconv1_stage4_L2/weights/read/_11__cf__11\n",
            "TfPoseEstimator/Mconv1_stage5_L1/biases/read/_12__cf__12\n",
            "TfPoseEstimator/Mconv1_stage5_L1/weights/read/_13__cf__13\n",
            "TfPoseEstimator/Mconv1_stage5_L2/biases/read/_14__cf__14\n",
            "TfPoseEstimator/Mconv1_stage5_L2/weights/read/_15__cf__15\n",
            "TfPoseEstimator/Mconv1_stage6_L1/biases/read/_16__cf__16\n",
            "TfPoseEstimator/Mconv1_stage6_L1/weights/read/_17__cf__17\n",
            "TfPoseEstimator/Mconv1_stage6_L2/biases/read/_18__cf__18\n",
            "TfPoseEstimator/Mconv1_stage6_L2/weights/read/_19__cf__19\n",
            "TfPoseEstimator/Mconv2_stage2_L1/biases/read/_20__cf__20\n",
            "TfPoseEstimator/Mconv2_stage2_L1/weights/read/_21__cf__21\n",
            "TfPoseEstimator/Mconv2_stage2_L2/biases/read/_22__cf__22\n",
            "TfPoseEstimator/Mconv2_stage2_L2/weights/read/_23__cf__23\n",
            "TfPoseEstimator/Mconv2_stage3_L1/biases/read/_24__cf__24\n",
            "TfPoseEstimator/Mconv2_stage3_L1/weights/read/_25__cf__25\n",
            "TfPoseEstimator/Mconv2_stage3_L2/biases/read/_26__cf__26\n",
            "TfPoseEstimator/Mconv2_stage3_L2/weights/read/_27__cf__27\n",
            "TfPoseEstimator/Mconv2_stage4_L1/biases/read/_28__cf__28\n",
            "TfPoseEstimator/Mconv2_stage4_L1/weights/read/_29__cf__29\n",
            "TfPoseEstimator/Mconv2_stage4_L2/biases/read/_30__cf__30\n",
            "TfPoseEstimator/Mconv2_stage4_L2/weights/read/_31__cf__31\n",
            "TfPoseEstimator/Mconv2_stage5_L1/biases/read/_32__cf__32\n",
            "TfPoseEstimator/Mconv2_stage5_L1/weights/read/_33__cf__33\n",
            "TfPoseEstimator/Mconv2_stage5_L2/biases/read/_34__cf__34\n",
            "TfPoseEstimator/Mconv2_stage5_L2/weights/read/_35__cf__35\n",
            "TfPoseEstimator/Mconv2_stage6_L1/biases/read/_36__cf__36\n",
            "TfPoseEstimator/Mconv2_stage6_L1/weights/read/_37__cf__37\n",
            "TfPoseEstimator/Mconv2_stage6_L2/biases/read/_38__cf__38\n",
            "TfPoseEstimator/Mconv2_stage6_L2/weights/read/_39__cf__39\n",
            "TfPoseEstimator/Mconv3_stage2_L1/biases/read/_40__cf__40\n",
            "TfPoseEstimator/Mconv3_stage2_L1/weights/read/_41__cf__41\n",
            "TfPoseEstimator/Mconv3_stage2_L2/biases/read/_42__cf__42\n",
            "TfPoseEstimator/Mconv3_stage2_L2/weights/read/_43__cf__43\n",
            "TfPoseEstimator/Mconv3_stage3_L1/biases/read/_44__cf__44\n",
            "TfPoseEstimator/Mconv3_stage3_L1/weights/read/_45__cf__45\n",
            "TfPoseEstimator/Mconv3_stage3_L2/biases/read/_46__cf__46\n",
            "TfPoseEstimator/Mconv3_stage3_L2/weights/read/_47__cf__47\n",
            "TfPoseEstimator/Mconv3_stage4_L1/biases/read/_48__cf__48\n",
            "TfPoseEstimator/Mconv3_stage4_L1/weights/read/_49__cf__49\n",
            "TfPoseEstimator/Mconv3_stage4_L2/biases/read/_50__cf__50\n",
            "TfPoseEstimator/Mconv3_stage4_L2/weights/read/_51__cf__51\n",
            "TfPoseEstimator/Mconv3_stage5_L1/biases/read/_52__cf__52\n",
            "TfPoseEstimator/Mconv3_stage5_L1/weights/read/_53__cf__53\n",
            "TfPoseEstimator/Mconv3_stage5_L2/biases/read/_54__cf__54\n",
            "TfPoseEstimator/Mconv3_stage5_L2/weights/read/_55__cf__55\n",
            "TfPoseEstimator/Mconv3_stage6_L1/biases/read/_56__cf__56\n",
            "TfPoseEstimator/Mconv3_stage6_L1/weights/read/_57__cf__57\n",
            "TfPoseEstimator/Mconv3_stage6_L2/biases/read/_58__cf__58\n",
            "TfPoseEstimator/Mconv3_stage6_L2/weights/read/_59__cf__59\n",
            "TfPoseEstimator/Mconv4_stage2_L1/biases/read/_60__cf__60\n",
            "TfPoseEstimator/Mconv4_stage2_L1/weights/read/_61__cf__61\n",
            "TfPoseEstimator/Mconv4_stage2_L2/biases/read/_62__cf__62\n",
            "TfPoseEstimator/Mconv4_stage2_L2/weights/read/_63__cf__63\n",
            "TfPoseEstimator/Mconv4_stage3_L1/biases/read/_64__cf__64\n",
            "TfPoseEstimator/Mconv4_stage3_L1/weights/read/_65__cf__65\n",
            "TfPoseEstimator/Mconv4_stage3_L2/biases/read/_66__cf__66\n",
            "TfPoseEstimator/Mconv4_stage3_L2/weights/read/_67__cf__67\n",
            "TfPoseEstimator/Mconv4_stage4_L1/biases/read/_68__cf__68\n",
            "TfPoseEstimator/Mconv4_stage4_L1/weights/read/_69__cf__69\n",
            "TfPoseEstimator/Mconv4_stage4_L2/biases/read/_70__cf__70\n",
            "TfPoseEstimator/Mconv4_stage4_L2/weights/read/_71__cf__71\n",
            "TfPoseEstimator/Mconv4_stage5_L1/biases/read/_72__cf__72\n",
            "TfPoseEstimator/Mconv4_stage5_L1/weights/read/_73__cf__73\n",
            "TfPoseEstimator/Mconv4_stage5_L2/biases/read/_74__cf__74\n",
            "TfPoseEstimator/Mconv4_stage5_L2/weights/read/_75__cf__75\n",
            "TfPoseEstimator/Mconv4_stage6_L1/biases/read/_76__cf__76\n",
            "TfPoseEstimator/Mconv4_stage6_L1/weights/read/_77__cf__77\n",
            "TfPoseEstimator/Mconv4_stage6_L2/biases/read/_78__cf__78\n",
            "TfPoseEstimator/Mconv4_stage6_L2/weights/read/_79__cf__79\n",
            "TfPoseEstimator/Mconv5_stage2_L1/biases/read/_80__cf__80\n",
            "TfPoseEstimator/Mconv5_stage2_L1/weights/read/_81__cf__81\n",
            "TfPoseEstimator/Mconv5_stage2_L2/biases/read/_82__cf__82\n",
            "TfPoseEstimator/Mconv5_stage2_L2/weights/read/_83__cf__83\n",
            "TfPoseEstimator/Mconv5_stage3_L1/biases/read/_84__cf__84\n",
            "TfPoseEstimator/Mconv5_stage3_L1/weights/read/_85__cf__85\n",
            "TfPoseEstimator/Mconv5_stage3_L2/biases/read/_86__cf__86\n",
            "TfPoseEstimator/Mconv5_stage3_L2/weights/read/_87__cf__87\n",
            "TfPoseEstimator/Mconv5_stage4_L1/biases/read/_88__cf__88\n",
            "TfPoseEstimator/Mconv5_stage4_L1/weights/read/_89__cf__89\n",
            "TfPoseEstimator/Mconv5_stage4_L2/biases/read/_90__cf__90\n",
            "TfPoseEstimator/Mconv5_stage4_L2/weights/read/_91__cf__91\n",
            "TfPoseEstimator/Mconv5_stage5_L1/biases/read/_92__cf__92\n",
            "TfPoseEstimator/Mconv5_stage5_L1/weights/read/_93__cf__93\n",
            "TfPoseEstimator/Mconv5_stage5_L2/biases/read/_94__cf__94\n",
            "TfPoseEstimator/Mconv5_stage5_L2/weights/read/_95__cf__95\n",
            "TfPoseEstimator/Mconv5_stage6_L1/biases/read/_96__cf__96\n",
            "TfPoseEstimator/Mconv5_stage6_L1/weights/read/_97__cf__97\n",
            "TfPoseEstimator/Mconv5_stage6_L2/biases/read/_98__cf__98\n",
            "TfPoseEstimator/Mconv5_stage6_L2/weights/read/_99__cf__99\n",
            "TfPoseEstimator/Mconv6_stage2_L1/biases/read/_100__cf__100\n",
            "TfPoseEstimator/Mconv6_stage2_L1/weights/read/_101__cf__101\n",
            "TfPoseEstimator/Mconv6_stage2_L2/biases/read/_102__cf__102\n",
            "TfPoseEstimator/Mconv6_stage2_L2/weights/read/_103__cf__103\n",
            "TfPoseEstimator/Mconv6_stage3_L1/biases/read/_104__cf__104\n",
            "TfPoseEstimator/Mconv6_stage3_L1/weights/read/_105__cf__105\n",
            "TfPoseEstimator/Mconv6_stage3_L2/biases/read/_106__cf__106\n",
            "TfPoseEstimator/Mconv6_stage3_L2/weights/read/_107__cf__107\n",
            "TfPoseEstimator/Mconv6_stage4_L1/biases/read/_108__cf__108\n",
            "TfPoseEstimator/Mconv6_stage4_L1/weights/read/_109__cf__109\n",
            "TfPoseEstimator/Mconv6_stage4_L2/biases/read/_110__cf__110\n",
            "TfPoseEstimator/Mconv6_stage4_L2/weights/read/_111__cf__111\n",
            "TfPoseEstimator/Mconv6_stage5_L1/biases/read/_112__cf__112\n",
            "TfPoseEstimator/Mconv6_stage5_L1/weights/read/_113__cf__113\n",
            "TfPoseEstimator/Mconv6_stage5_L2/biases/read/_114__cf__114\n",
            "TfPoseEstimator/Mconv6_stage5_L2/weights/read/_115__cf__115\n",
            "TfPoseEstimator/Mconv6_stage6_L1/biases/read/_116__cf__116\n",
            "TfPoseEstimator/Mconv6_stage6_L1/weights/read/_117__cf__117\n",
            "TfPoseEstimator/Mconv6_stage6_L2/biases/read/_118__cf__118\n",
            "TfPoseEstimator/Mconv6_stage6_L2/weights/read/_119__cf__119\n",
            "TfPoseEstimator/Mconv7_stage2_L1/biases/read/_120__cf__120\n",
            "TfPoseEstimator/Mconv7_stage2_L1/weights/read/_121__cf__121\n",
            "TfPoseEstimator/Mconv7_stage2_L2/biases/read/_122__cf__122\n",
            "TfPoseEstimator/Mconv7_stage2_L2/weights/read/_123__cf__123\n",
            "TfPoseEstimator/Mconv7_stage3_L1/biases/read/_124__cf__124\n",
            "TfPoseEstimator/Mconv7_stage3_L1/weights/read/_125__cf__125\n",
            "TfPoseEstimator/Mconv7_stage3_L2/biases/read/_126__cf__126\n",
            "TfPoseEstimator/Mconv7_stage3_L2/weights/read/_127__cf__127\n",
            "TfPoseEstimator/Mconv7_stage4_L1/biases/read/_128__cf__128\n",
            "TfPoseEstimator/Mconv7_stage4_L1/weights/read/_129__cf__129\n",
            "TfPoseEstimator/Mconv7_stage4_L2/biases/read/_130__cf__130\n",
            "TfPoseEstimator/Mconv7_stage4_L2/weights/read/_131__cf__131\n",
            "TfPoseEstimator/Mconv7_stage5_L1/biases/read/_132__cf__132\n",
            "TfPoseEstimator/Mconv7_stage5_L1/weights/read/_133__cf__133\n",
            "TfPoseEstimator/Mconv7_stage5_L2/biases/read/_134__cf__134\n",
            "TfPoseEstimator/Mconv7_stage5_L2/weights/read/_135__cf__135\n",
            "TfPoseEstimator/Mconv7_stage6_L1/biases/read/_136__cf__136\n",
            "TfPoseEstimator/Mconv7_stage6_L1/weights/read/_137__cf__137\n",
            "TfPoseEstimator/Mconv7_stage6_L2/biases/read/_138__cf__138\n",
            "TfPoseEstimator/Mconv7_stage6_L2/weights/read/_139__cf__139\n",
            "TfPoseEstimator/conv1_1/biases/read/_140__cf__140\n",
            "TfPoseEstimator/conv1_1/weights/read/_141__cf__141\n",
            "TfPoseEstimator/conv1_2/biases/read/_142__cf__142\n",
            "TfPoseEstimator/conv1_2/weights/read/_143__cf__143\n",
            "TfPoseEstimator/conv2_1/biases/read/_144__cf__144\n",
            "TfPoseEstimator/conv2_1/weights/read/_145__cf__145\n",
            "TfPoseEstimator/conv2_2/biases/read/_146__cf__146\n",
            "TfPoseEstimator/conv2_2/weights/read/_147__cf__147\n",
            "TfPoseEstimator/conv3_1/biases/read/_148__cf__148\n",
            "TfPoseEstimator/conv3_1/weights/read/_149__cf__149\n",
            "TfPoseEstimator/conv3_2/biases/read/_150__cf__150\n",
            "TfPoseEstimator/conv3_2/weights/read/_151__cf__151\n",
            "TfPoseEstimator/conv3_3/biases/read/_152__cf__152\n",
            "TfPoseEstimator/conv3_3/weights/read/_153__cf__153\n",
            "TfPoseEstimator/conv3_4/biases/read/_154__cf__154\n",
            "TfPoseEstimator/conv3_4/weights/read/_155__cf__155\n",
            "TfPoseEstimator/conv4_1/biases/read/_156__cf__156\n",
            "TfPoseEstimator/conv4_1/weights/read/_157__cf__157\n",
            "TfPoseEstimator/conv4_2/biases/read/_158__cf__158\n",
            "TfPoseEstimator/conv4_2/weights/read/_159__cf__159\n",
            "TfPoseEstimator/conv4_3_CPM/biases/read/_160__cf__160\n",
            "TfPoseEstimator/conv4_3_CPM/weights/read/_161__cf__161\n",
            "TfPoseEstimator/conv4_4_CPM/biases/read/_162__cf__162\n",
            "TfPoseEstimator/conv4_4_CPM/weights/read/_163__cf__163\n",
            "TfPoseEstimator/conv5_1_CPM_L1/biases/read/_164__cf__164\n",
            "TfPoseEstimator/conv5_1_CPM_L1/weights/read/_165__cf__165\n",
            "TfPoseEstimator/conv5_1_CPM_L2/biases/read/_166__cf__166\n",
            "TfPoseEstimator/conv5_1_CPM_L2/weights/read/_167__cf__167\n",
            "TfPoseEstimator/conv5_2_CPM_L1/biases/read/_168__cf__168\n",
            "TfPoseEstimator/conv5_2_CPM_L1/weights/read/_169__cf__169\n",
            "TfPoseEstimator/conv5_2_CPM_L2/biases/read/_170__cf__170\n",
            "TfPoseEstimator/conv5_2_CPM_L2/weights/read/_171__cf__171\n",
            "TfPoseEstimator/conv5_3_CPM_L1/biases/read/_172__cf__172\n",
            "TfPoseEstimator/conv5_3_CPM_L1/weights/read/_173__cf__173\n",
            "TfPoseEstimator/conv5_3_CPM_L2/biases/read/_174__cf__174\n",
            "TfPoseEstimator/conv5_3_CPM_L2/weights/read/_175__cf__175\n",
            "TfPoseEstimator/conv5_4_CPM_L1/biases/read/_176__cf__176\n",
            "TfPoseEstimator/conv5_4_CPM_L1/weights/read/_177__cf__177\n",
            "TfPoseEstimator/conv5_4_CPM_L2/biases/read/_178__cf__178\n",
            "TfPoseEstimator/conv5_4_CPM_L2/weights/read/_179__cf__179\n",
            "TfPoseEstimator/conv5_5_CPM_L1/biases/read/_180__cf__180\n",
            "TfPoseEstimator/conv5_5_CPM_L1/weights/read/_181__cf__181\n",
            "TfPoseEstimator/conv5_5_CPM_L2/biases/read/_182__cf__182\n",
            "TfPoseEstimator/conv5_5_CPM_L2/weights/read/_183__cf__183\n",
            "TfPoseEstimator/image\n",
            "TfPoseEstimator/preprocess_divide\n",
            "TfPoseEstimator/preprocess_subtract\n",
            "TfPoseEstimator/conv1_1/Conv2D\n",
            "TfPoseEstimator/conv1_1/BiasAdd\n",
            "TfPoseEstimator/conv1_1/conv1_1\n",
            "TfPoseEstimator/conv1_2/Conv2D\n",
            "TfPoseEstimator/conv1_2/BiasAdd\n",
            "TfPoseEstimator/conv1_2/conv1_2\n",
            "TfPoseEstimator/pool1_stage1\n",
            "TfPoseEstimator/conv2_1/Conv2D\n",
            "TfPoseEstimator/conv2_1/BiasAdd\n",
            "TfPoseEstimator/conv2_1/conv2_1\n",
            "TfPoseEstimator/conv2_2/Conv2D\n",
            "TfPoseEstimator/conv2_2/BiasAdd\n",
            "TfPoseEstimator/conv2_2/conv2_2\n",
            "TfPoseEstimator/pool2_stage1\n",
            "TfPoseEstimator/conv3_1/Conv2D\n",
            "TfPoseEstimator/conv3_1/BiasAdd\n",
            "TfPoseEstimator/conv3_1/conv3_1\n",
            "TfPoseEstimator/conv3_2/Conv2D\n",
            "TfPoseEstimator/conv3_2/BiasAdd\n",
            "TfPoseEstimator/conv3_2/conv3_2\n",
            "TfPoseEstimator/conv3_3/Conv2D\n",
            "TfPoseEstimator/conv3_3/BiasAdd\n",
            "TfPoseEstimator/conv3_3/conv3_3\n",
            "TfPoseEstimator/conv3_4/Conv2D\n",
            "TfPoseEstimator/conv3_4/BiasAdd\n",
            "TfPoseEstimator/conv3_4/conv3_4\n",
            "TfPoseEstimator/pool3_stage1\n",
            "TfPoseEstimator/conv4_1/Conv2D\n",
            "TfPoseEstimator/conv4_1/BiasAdd\n",
            "TfPoseEstimator/conv4_1/conv4_1\n",
            "TfPoseEstimator/conv4_2/Conv2D\n",
            "TfPoseEstimator/conv4_2/BiasAdd\n",
            "TfPoseEstimator/conv4_2/conv4_2\n",
            "TfPoseEstimator/conv4_3_CPM/Conv2D\n",
            "TfPoseEstimator/conv4_3_CPM/BiasAdd\n",
            "TfPoseEstimator/conv4_3_CPM/conv4_3_CPM\n",
            "TfPoseEstimator/conv4_4_CPM/Conv2D\n",
            "TfPoseEstimator/conv4_4_CPM/BiasAdd\n",
            "TfPoseEstimator/conv4_4_CPM/conv4_4_CPM\n",
            "TfPoseEstimator/conv5_1_CPM_L1/Conv2D\n",
            "TfPoseEstimator/conv5_1_CPM_L1/BiasAdd\n",
            "TfPoseEstimator/conv5_1_CPM_L1/conv5_1_CPM_L1\n",
            "TfPoseEstimator/conv5_2_CPM_L1/Conv2D\n",
            "TfPoseEstimator/conv5_2_CPM_L1/BiasAdd\n",
            "TfPoseEstimator/conv5_2_CPM_L1/conv5_2_CPM_L1\n",
            "TfPoseEstimator/conv5_3_CPM_L1/Conv2D\n",
            "TfPoseEstimator/conv5_3_CPM_L1/BiasAdd\n",
            "TfPoseEstimator/conv5_3_CPM_L1/conv5_3_CPM_L1\n",
            "TfPoseEstimator/conv5_4_CPM_L1/Conv2D\n",
            "TfPoseEstimator/conv5_4_CPM_L1/BiasAdd\n",
            "TfPoseEstimator/conv5_4_CPM_L1/conv5_4_CPM_L1\n",
            "TfPoseEstimator/conv5_5_CPM_L1/Conv2D\n",
            "TfPoseEstimator/conv5_5_CPM_L1/BiasAdd\n",
            "TfPoseEstimator/conv5_1_CPM_L2/Conv2D\n",
            "TfPoseEstimator/conv5_1_CPM_L2/BiasAdd\n",
            "TfPoseEstimator/conv5_1_CPM_L2/conv5_1_CPM_L2\n",
            "TfPoseEstimator/conv5_2_CPM_L2/Conv2D\n",
            "TfPoseEstimator/conv5_2_CPM_L2/BiasAdd\n",
            "TfPoseEstimator/conv5_2_CPM_L2/conv5_2_CPM_L2\n",
            "TfPoseEstimator/conv5_3_CPM_L2/Conv2D\n",
            "TfPoseEstimator/conv5_3_CPM_L2/BiasAdd\n",
            "TfPoseEstimator/conv5_3_CPM_L2/conv5_3_CPM_L2\n",
            "TfPoseEstimator/conv5_4_CPM_L2/Conv2D\n",
            "TfPoseEstimator/conv5_4_CPM_L2/BiasAdd\n",
            "TfPoseEstimator/conv5_4_CPM_L2/conv5_4_CPM_L2\n",
            "TfPoseEstimator/conv5_5_CPM_L2/Conv2D\n",
            "TfPoseEstimator/conv5_5_CPM_L2/BiasAdd\n",
            "TfPoseEstimator/concat_stage2\n",
            "TfPoseEstimator/Mconv1_stage2_L1/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage2_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage2_L1/Mconv1_stage2_L1\n",
            "TfPoseEstimator/Mconv2_stage2_L1/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage2_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage2_L1/Mconv2_stage2_L1\n",
            "TfPoseEstimator/Mconv3_stage2_L1/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage2_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage2_L1/Mconv3_stage2_L1\n",
            "TfPoseEstimator/Mconv4_stage2_L1/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage2_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage2_L1/Mconv4_stage2_L1\n",
            "TfPoseEstimator/Mconv5_stage2_L1/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage2_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage2_L1/Mconv5_stage2_L1\n",
            "TfPoseEstimator/Mconv6_stage2_L1/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage2_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage2_L1/Mconv6_stage2_L1\n",
            "TfPoseEstimator/Mconv7_stage2_L1/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage2_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage2_L2/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage2_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage2_L2/Mconv1_stage2_L2\n",
            "TfPoseEstimator/Mconv2_stage2_L2/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage2_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage2_L2/Mconv2_stage2_L2\n",
            "TfPoseEstimator/Mconv3_stage2_L2/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage2_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage2_L2/Mconv3_stage2_L2\n",
            "TfPoseEstimator/Mconv4_stage2_L2/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage2_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage2_L2/Mconv4_stage2_L2\n",
            "TfPoseEstimator/Mconv5_stage2_L2/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage2_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage2_L2/Mconv5_stage2_L2\n",
            "TfPoseEstimator/Mconv6_stage2_L2/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage2_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage2_L2/Mconv6_stage2_L2\n",
            "TfPoseEstimator/Mconv7_stage2_L2/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage2_L2/BiasAdd\n",
            "TfPoseEstimator/concat_stage3\n",
            "TfPoseEstimator/Mconv1_stage3_L1/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage3_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage3_L1/Mconv1_stage3_L1\n",
            "TfPoseEstimator/Mconv2_stage3_L1/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage3_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage3_L1/Mconv2_stage3_L1\n",
            "TfPoseEstimator/Mconv3_stage3_L1/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage3_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage3_L1/Mconv3_stage3_L1\n",
            "TfPoseEstimator/Mconv4_stage3_L1/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage3_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage3_L1/Mconv4_stage3_L1\n",
            "TfPoseEstimator/Mconv5_stage3_L1/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage3_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage3_L1/Mconv5_stage3_L1\n",
            "TfPoseEstimator/Mconv6_stage3_L1/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage3_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage3_L1/Mconv6_stage3_L1\n",
            "TfPoseEstimator/Mconv7_stage3_L1/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage3_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage3_L2/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage3_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage3_L2/Mconv1_stage3_L2\n",
            "TfPoseEstimator/Mconv2_stage3_L2/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage3_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage3_L2/Mconv2_stage3_L2\n",
            "TfPoseEstimator/Mconv3_stage3_L2/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage3_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage3_L2/Mconv3_stage3_L2\n",
            "TfPoseEstimator/Mconv4_stage3_L2/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage3_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage3_L2/Mconv4_stage3_L2\n",
            "TfPoseEstimator/Mconv5_stage3_L2/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage3_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage3_L2/Mconv5_stage3_L2\n",
            "TfPoseEstimator/Mconv6_stage3_L2/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage3_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage3_L2/Mconv6_stage3_L2\n",
            "TfPoseEstimator/Mconv7_stage3_L2/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage3_L2/BiasAdd\n",
            "TfPoseEstimator/concat_stage4\n",
            "TfPoseEstimator/Mconv1_stage4_L1/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage4_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage4_L1/Mconv1_stage4_L1\n",
            "TfPoseEstimator/Mconv2_stage4_L1/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage4_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage4_L1/Mconv2_stage4_L1\n",
            "TfPoseEstimator/Mconv3_stage4_L1/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage4_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage4_L1/Mconv3_stage4_L1\n",
            "TfPoseEstimator/Mconv4_stage4_L1/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage4_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage4_L1/Mconv4_stage4_L1\n",
            "TfPoseEstimator/Mconv5_stage4_L1/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage4_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage4_L1/Mconv5_stage4_L1\n",
            "TfPoseEstimator/Mconv6_stage4_L1/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage4_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage4_L1/Mconv6_stage4_L1\n",
            "TfPoseEstimator/Mconv7_stage4_L1/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage4_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage4_L2/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage4_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage4_L2/Mconv1_stage4_L2\n",
            "TfPoseEstimator/Mconv2_stage4_L2/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage4_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage4_L2/Mconv2_stage4_L2\n",
            "TfPoseEstimator/Mconv3_stage4_L2/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage4_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage4_L2/Mconv3_stage4_L2\n",
            "TfPoseEstimator/Mconv4_stage4_L2/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage4_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage4_L2/Mconv4_stage4_L2\n",
            "TfPoseEstimator/Mconv5_stage4_L2/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage4_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage4_L2/Mconv5_stage4_L2\n",
            "TfPoseEstimator/Mconv6_stage4_L2/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage4_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage4_L2/Mconv6_stage4_L2\n",
            "TfPoseEstimator/Mconv7_stage4_L2/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage4_L2/BiasAdd\n",
            "TfPoseEstimator/concat_stage5\n",
            "TfPoseEstimator/Mconv1_stage5_L1/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage5_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage5_L1/Mconv1_stage5_L1\n",
            "TfPoseEstimator/Mconv2_stage5_L1/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage5_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage5_L1/Mconv2_stage5_L1\n",
            "TfPoseEstimator/Mconv3_stage5_L1/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage5_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage5_L1/Mconv3_stage5_L1\n",
            "TfPoseEstimator/Mconv4_stage5_L1/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage5_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage5_L1/Mconv4_stage5_L1\n",
            "TfPoseEstimator/Mconv5_stage5_L1/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage5_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage5_L1/Mconv5_stage5_L1\n",
            "TfPoseEstimator/Mconv6_stage5_L1/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage5_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage5_L1/Mconv6_stage5_L1\n",
            "TfPoseEstimator/Mconv7_stage5_L1/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage5_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage5_L2/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage5_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage5_L2/Mconv1_stage5_L2\n",
            "TfPoseEstimator/Mconv2_stage5_L2/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage5_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage5_L2/Mconv2_stage5_L2\n",
            "TfPoseEstimator/Mconv3_stage5_L2/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage5_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage5_L2/Mconv3_stage5_L2\n",
            "TfPoseEstimator/Mconv4_stage5_L2/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage5_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage5_L2/Mconv4_stage5_L2\n",
            "TfPoseEstimator/Mconv5_stage5_L2/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage5_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage5_L2/Mconv5_stage5_L2\n",
            "TfPoseEstimator/Mconv6_stage5_L2/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage5_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage5_L2/Mconv6_stage5_L2\n",
            "TfPoseEstimator/Mconv7_stage5_L2/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage5_L2/BiasAdd\n",
            "TfPoseEstimator/concat_stage6\n",
            "TfPoseEstimator/Mconv1_stage6_L1/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage6_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage6_L1/Mconv1_stage6_L1\n",
            "TfPoseEstimator/Mconv2_stage6_L1/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage6_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage6_L1/Mconv2_stage6_L1\n",
            "TfPoseEstimator/Mconv3_stage6_L1/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage6_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage6_L1/Mconv3_stage6_L1\n",
            "TfPoseEstimator/Mconv4_stage6_L1/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage6_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage6_L1/Mconv4_stage6_L1\n",
            "TfPoseEstimator/Mconv5_stage6_L1/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage6_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage6_L1/Mconv5_stage6_L1\n",
            "TfPoseEstimator/Mconv6_stage6_L1/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage6_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage6_L1/Mconv6_stage6_L1\n",
            "TfPoseEstimator/Mconv7_stage6_L1/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage6_L1/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage6_L2/Conv2D\n",
            "TfPoseEstimator/Mconv1_stage6_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv1_stage6_L2/Mconv1_stage6_L2\n",
            "TfPoseEstimator/Mconv2_stage6_L2/Conv2D\n",
            "TfPoseEstimator/Mconv2_stage6_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv2_stage6_L2/Mconv2_stage6_L2\n",
            "TfPoseEstimator/Mconv3_stage6_L2/Conv2D\n",
            "TfPoseEstimator/Mconv3_stage6_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv3_stage6_L2/Mconv3_stage6_L2\n",
            "TfPoseEstimator/Mconv4_stage6_L2/Conv2D\n",
            "TfPoseEstimator/Mconv4_stage6_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv4_stage6_L2/Mconv4_stage6_L2\n",
            "TfPoseEstimator/Mconv5_stage6_L2/Conv2D\n",
            "TfPoseEstimator/Mconv5_stage6_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv5_stage6_L2/Mconv5_stage6_L2\n",
            "TfPoseEstimator/Mconv6_stage6_L2/Conv2D\n",
            "TfPoseEstimator/Mconv6_stage6_L2/BiasAdd\n",
            "TfPoseEstimator/Mconv6_stage6_L2/Mconv6_stage6_L2\n",
            "TfPoseEstimator/Mconv7_stage6_L2/Conv2D\n",
            "TfPoseEstimator/Mconv7_stage6_L2/BiasAdd\n",
            "TfPoseEstimator/Openpose/concat_stage7\n",
            "HEEEEJ\n",
            "Tensor(\"strided_slice_2:0\", shape=(?, ?, ?, 19), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0425 09:41:37.064394 140561405978432 deprecation.py:641] From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/deprecation.py:576: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`normal` is a deprecated alias for `truncated_normal`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PID\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0425 09:41:37.237167 140561405978432 deprecation.py:364] From /usr/local/lib/python3.9/dist-packages/keras/layers/normalization/batch_normalization.py:581: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zZrlYUP1Qkr",
        "outputId": "ffa10c3d-d094-4c03-8566-9990809868ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import csv\n",
        "# Run this\n",
        "FLAGS.yolo_iou_threshold = 0.5\n",
        "FLAGS.yolo_score_threshold = 0.5\n",
        "\n",
        "color = (255, 0, 0) \n",
        "thickness = 2\n",
        "\n",
        "yolo = YoloV3(classes=FLAGS.num_classes)\n",
        "\n",
        "yolo.load_weights(FLAGS.weights).expect_partial()\n",
        "logging.info('weights loaded')\n",
        "\n",
        "class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n",
        "logging.info('classes loaded')\n",
        "\n",
        "resize_out_ratio = 4.0\n",
        "\n",
        "def run_model():\n",
        "\n",
        "  print('Processing started.......')\n",
        "\n",
        "  fps_time = 0\n",
        "\n",
        "  try:\n",
        "      vid = cv2.VideoCapture(int(FLAGS.video))\n",
        "  except:\n",
        "      vid = cv2.VideoCapture(FLAGS.video)\n",
        "\n",
        "  out = None\n",
        "\n",
        "  if FLAGS.output:\n",
        "      # by default VideoCapture returns float instead of int\n",
        "      width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "      height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "      fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
        "      codec = cv2.VideoWriter_fourcc(*FLAGS.output_format)\n",
        "      out = cv2.VideoWriter(FLAGS.output, codec, fps, (width, height))\n",
        "\n",
        "  frame = 0\n",
        "  rolling_data={}\n",
        "  fps_time=0\n",
        "  result=[]\n",
        "  csv_file = \"pedestrian_data.csv\"\n",
        "  header = [\"Frame Number\", \"Pedestrian ID\",\"distance\", \"BCoordinates x\", \"BCoordinates y\", \"BCoordinates wx\", \"BCoordinates hy\",\"Pedestrian Intent\"]\n",
        "  data_dict={}\n",
        "  \n",
        "\n",
        "  while True:\n",
        "\n",
        "    _, img = vid.read() # reading the image\n",
        "\n",
        "    if img is None:\n",
        "        break\n",
        "        logging.warning(\"Empty Frame\")\n",
        "        time.sleep(0.1)\n",
        "        continue\n",
        "    frame += 1   \n",
        "    currFrame = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "    fps = vid.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "\n",
        "    img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
        "    img_orig = np.copy(img)\n",
        "    img_in = tf.expand_dims(img_in, 0)\n",
        "    img_in = transform_images(img_in, FLAGS.size)\n",
        "\n",
        "    t1 = time.time()\n",
        "    boxes, scores, classes, nums = yolo.predict(img_in, steps=1) # yolo\n",
        "    t2 = time.time()\n",
        "    dist_dict=[]  #to store distance of pedestrian form vehicle\n",
        "    \n",
        "    # for 80 cls yolo\n",
        "\n",
        "    boxes = boxes[:,:nums[0],:].reshape(nums[0], 4)[classes[0][:nums[0]] == 0]\n",
        "    scores = scores[0][:nums[0]][classes[0][:nums[0]] == 0]\n",
        "    nums = len(boxes)\n",
        "    print(boxes)\n",
        "\n",
        "    for i,b in enumerate(boxes[0]):\n",
        "      if classes[0][i]==0:\n",
        "        apx_dist=round((1-(boxes[0][i][3]-boxes[0][i][1]))**4,1)\n",
        "        dist_dict.append(apx_dist)\n",
        "\n",
        "    print(dist_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    wh = np.flip(img.shape[0:2])\n",
        "    bbtlwh = []\n",
        "    for i in range(nums):\n",
        "\n",
        "\n",
        "      x1y1 = tuple((np.array(boxes[i][0:2]) * wh).astype(np.int32))\n",
        "      x1 = x1y1[0]\n",
        "      y1 = x1y1[1]\n",
        "      x2y2 = tuple((np.array(boxes[i][2:4]) * wh).astype(np.int32))\n",
        "      bbwh = (x2y2[0]-x1y1[0], x2y2[1]-x1y1[1])\n",
        "      w = bbwh[0]\n",
        "      h = bbwh[1]\n",
        "      bbtlwh.append([x1,y1,w,h])\n",
        "\n",
        "    features = encoder(img, bbtlwh) # deepsort input\n",
        "    detections = [Detection(box, conf, feat) for box, conf, feat in zip(bbtlwh, scores, features)] #deep sort output \n",
        "\n",
        "    # Update tracker.\n",
        "    tracker.predict()\n",
        "    tracker.update(detections)\n",
        "    \n",
        "    tracked_bbox = []\n",
        "    ids = []\n",
        "\n",
        "    for track in tracker.tracks:\n",
        "\n",
        "      if not track.is_confirmed() or track.time_since_update > 1:\n",
        "        continue\n",
        "      tracked_bbox.append(track.to_tlwh())\n",
        "      ids.append(track.track_id)\n",
        "    \n",
        "    x_array = []\n",
        "    y_array = []\n",
        "    w_array = []\n",
        "    h_array = []\n",
        "    intent_array = []\n",
        "\n",
        "\n",
        "    for i in range(len(tracked_bbox)): # densenet \n",
        "\n",
        "      # Show tracker output\n",
        "      x, y, w, h = tracked_bbox[i]\n",
        "      x = int(x)  \n",
        "      y = int(y) \n",
        "      w = int(w) \n",
        "      h = int(h) \n",
        "\n",
        "  \n",
        "      x_array.append(x)\n",
        "      y_array.append(y)\n",
        "      w_array.append(w+x)\n",
        "      h_array.append(h+y)\n",
        "\n",
        "      # plot the skeletons\n",
        "      try:\n",
        "        cropped = img_orig[y:y + h, x:x + w]\n",
        "        humans = model.inference(cropped, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
        "        humans.sort(key=lambda human: human.score, reverse=True)\n",
        "        skelett = TfPoseEstimator.draw_humans(cropped, humans, imgcopy=True)\n",
        "        img_orig[y:y + h, x:x + w] = skelett\n",
        "        img_orig2 = img_orig\n",
        "      \n",
        "      except:\n",
        "        img_orig2 = img_orig\n",
        "\n",
        "      # looking for previous 16 frames data for a given pedestrian:\n",
        "\n",
        "      intent = 0 #(default, the pedestrian is not crossing)\n",
        "\n",
        "      \n",
        "      if int(ids[i]) in list(rolling_data.keys()):\n",
        "\n",
        "        if len(rolling_data[int(ids[i])]) == 16:\n",
        "          \n",
        "          seq = np.stack(np.array(rolling_data[int(ids[i])]),axis=2)\n",
        "          seq = np.expand_dims(seq, axis=0)\n",
        "          intent = pred_func(seq) # classification output\n",
        "\n",
        "        else:\n",
        "\n",
        "          seq = np.stack(np.array([rolling_data[int(ids[i])][-1]] * 16),axis=2)\n",
        "          seq = np.expand_dims(seq, axis=0)\n",
        "          intent = pred_func(seq) # classification output\n",
        "      \n",
        "\n",
        "      # risky pedestrian identification thru box color\n",
        "      intent_array.append(intent)\n",
        "\n",
        "      if intent == 1:\n",
        "        color = (0, 0, 255) # Red -> Crossing\n",
        "\n",
        "      else:\n",
        "        color = (0, 255, 0) # green -> Not crossing\n",
        "\n",
        "      fps_time = time.time()\n",
        "      img = cv2.rectangle(img_orig2, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n",
        "      img = cv2.putText(img, 'TrackID ' + str(ids[i]), (x, y - 5), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 0, 0), thickness=2)\n",
        "      img = cv2.putText(img,\"Frame No: %d\" % (frame),(10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 0, 255), 2)\n",
        "      result.append([frame, str(ids), str(dist_dict),str(x_array), str(y_array), str(w_array),str(h_array), str(intent_array)])\n",
        "      print(result)\n",
        "\n",
        "# Loop through each row of data\n",
        "      for row in result:\n",
        "        label = row[0]\n",
        "    \n",
        "      # Update the dictionary with the current row of data for the given label\n",
        "      data_dict[label] = row\n",
        "\n",
        "\n",
        "\n",
        "      with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # Write each row of data to the CSV file\n",
        "        for label in sorted(data_dict.keys()):\n",
        "          writer.writerow(data_dict[label])\n",
        "        # for row in result:\n",
        "        #   writer.writerow(row)\n",
        "\n",
        "      # storing the data for last 16 frames\n",
        "\n",
        "      try:\n",
        "\n",
        "        if int(ids[i]) in list(rolling_data.keys()): # ID exists in dict\n",
        "\n",
        "          if len(rolling_data[int(ids[i])]) < 16: # bboxes values for 16 frames\n",
        "              \n",
        "            cropped_seq = []\n",
        "            cropped_img = cv2.resize(img_orig[y:h+y, x:w+x],(100,100))\n",
        "            rolling_data[int(ids[i])].append(np.asarray(cropped_img)) # append the image      \n",
        "\n",
        "          else:\n",
        "\n",
        "            del rolling_data[int(ids[i])][0] # delete oldest frame bbox and append latest frame bbox\n",
        "            cropped_seq = []\n",
        "            cropped_img = cv2.resize(img_orig[y:h+y, x:w+x],(100,100))\n",
        "            rolling_data[int(ids[i])].append(np.asarray(cropped_img))\n",
        "              \n",
        "        else:\n",
        "\n",
        "          cropped_seq = []\n",
        "          cropped_img = cv2.resize(img_orig[y:h+y, x:w+x],(100,100))\n",
        "          rolling_data[int(ids[i])] = [np.asarray(cropped_img)]  \n",
        "\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    \n",
        "    if FLAGS.output:\n",
        "      out.write(img)\n",
        "\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "      break\n",
        "\n",
        "  cv2.destroyAllWindows()\n",
        "  print('\\nProcessing completed.......!!!')\n",
        "  print('Check video file in PID folder!')\n",
        "\n",
        "  return"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I0425 10:05:52.242028 140561405978432 <ipython-input-8-92317e5acd4b>:12] weights loaded\n",
            "I0425 10:05:52.244339 140561405978432 <ipython-input-8-92317e5acd4b>:15] classes loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_model()"
      ],
      "metadata": {
        "id": "XOggjPQqkPdg",
        "outputId": "00d3c487-1525-4340-ab6b-319b6a459c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing started.......\n",
            "[[0.5987924  0.5943559  0.618751   0.66709155]\n",
            " [0.39004835 0.607336   0.40489236 0.6593372 ]\n",
            " [0.6247905  0.59742045 0.6522843  0.71502507]\n",
            " [0.33202893 0.6139322  0.34485602 0.6592242 ]\n",
            " [0.42853692 0.60801095 0.44098195 0.65693825]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d8af0fd933a9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-92317e5acd4b>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mapx_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mdist_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapx_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    }
  ]
}